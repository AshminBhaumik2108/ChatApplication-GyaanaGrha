from langchain_google_genai import GoogleGenerativeAI
from backend import config

# Get the Google Generative AI model instance......
def get_llm(temperature=None):
    # Allow dynamic temperature override.....
    temp = temperature if temperature is not None else config.TEMPERATURE
    try:
        # Initialize Google Generative AI model for the LLM response to run with the Vector DB for the file.........
        # This will allow the LLM to access the vector database for improved responses.....
        # This is important for providing more relevant and accurate answers to user queries based on the Embeddings.....
        return GoogleGenerativeAI(
            model=config.GEMINI_MODEL, # Model name of Gemini.....
            google_api_key=config.GOOGLE_API_KEY, # Developers API Key from the Google Cloud Console.....
            temperature=temp, # How creative should the Answers be like.....
            max_output_tokens=config.MAX_TOKENS # Maximum count of Words should be returned.....
        )
    # Handle quota exceeded or other API errors gracefully.....
    # This is important for providing a good user experience and avoiding crashes of the Streamlit app (React).....
    except Exception as e:
        # Error handling for the file.....
        import streamlit as st
        st.error(f"Error initializing Gemini LLM AshminBhaumik: {e}")
        raise
